{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "raw_sources = glob.glob('./helivan-project-generation/bstadt/tlm/coca/text/text*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_general(fpath):\n",
    "\n",
    "    with open(fpath, 'r', encoding='utf-8') as file:\n",
    "        print('loading text from: ', fpath)\n",
    "        text = file.read()\n",
    "\n",
    "    texts = []\n",
    "    year = fpath.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    text = text.replace('<p>', ' ')\n",
    "    text = text.replace('<h>', ' ')\n",
    "    text = text.replace('@!', ' ')\n",
    "    text = text.replace('@ @ @ @ @ @ @ @ @ @', '[MASK_NOLOSS]')\n",
    "    text = text.split('@@')\n",
    "    for e in text:\n",
    "        if e.strip():\n",
    "            # Find the first whitespace and split into 2 parts\n",
    "            first_space_index = e.find(' ')\n",
    "            if first_space_index != -1:\n",
    "                docid = e[:first_space_index]\n",
    "                content = '[YEAR:{year}] '.format(year=year) + e[first_space_index + 1:]\n",
    "                texts.append(content)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenum_to_year = {}\n",
    "with open('./helivan-project-generation/bstadt/tlm/coca/sources.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    for line in file:\n",
    "        parts = line.split('\\t')\n",
    "        try:\n",
    "            filenum_to_year[parts[0]] = int(parts[1])\n",
    "        except Exception as e:\n",
    "            print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_blog_web(fpath):\n",
    "\n",
    "    with open(fpath, 'r', encoding='utf-8') as file:\n",
    "        print('loading text from: ', fpath)\n",
    "        text = file.read()\n",
    "\n",
    "    texts = []\n",
    "    text = text.replace('<p>', ' ')\n",
    "    text = text.replace('<h>', ' ')\n",
    "    text = text.replace('&', ' ')\n",
    "    text = text.replace('@ @ @ @ @ @ @ @ @ @', '[MASK_NOLOSS]')\n",
    "    text = text.split('@@')\n",
    "    for e in text:\n",
    "        if e.strip():\n",
    "            # Find the first whitespace and split into 2 parts\n",
    "            first_space_index = e.find(' ')\n",
    "            if first_space_index != -1:\n",
    "                docid = e[:first_space_index]\n",
    "                text = e[first_space_index + 1:]\n",
    "                if docid in filenum_to_year:\n",
    "                    year = filenum_to_year[docid]\n",
    "                    content = '[YEAR:{year}] '.format(year=year) + e[first_space_index + 1:]\n",
    "                    texts.append(content)\n",
    "                else:\n",
    "                    print(f\"docid {docid} not found in filenum_to_year\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_file(fpath):\n",
    "\n",
    "\n",
    "    if 'acad' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'blog' in fpath:\n",
    "        texts = preproc_blog_web(fpath)\n",
    "    elif 'fic' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'mag' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'news' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'spok' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'tvm' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'web' in fpath:\n",
    "        texts = preproc_blog_web(fpath)\n",
    "\n",
    "\n",
    "    else:                \n",
    "        raise NotImplementedError(f\"Genre type not supported: {fpath}\")\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new special tokens\n",
    "additional_special_tokens = ['[MASK_NOLOSS]'] + ['[YEAR:{i}]'.format(i=i) for i in range(1900, 2025)]\n",
    "special_tokens_dict = {'additional_special_tokens': additional_special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_source(source):\n",
    "    sequence_length = 512\n",
    "    overlap = 128\n",
    "    step_size = sequence_length - overlap\n",
    "    for text in preproc_file(source):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        sequences = [tokens[i:i + sequence_length] for i in range(0, len(tokens), step_size)]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text from:  ./helivan-project-generation/bstadt/tlm/coca/text/text_acad_1990.txt\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenize_source(raw_sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 210 430 760]\n"
     ]
    }
   ],
   "source": [
    "def get_file_idx(cumulative_lengths, idx):\n",
    "    return int(np.searchsorted(cumulative_lengths, idx))\n",
    "\n",
    "import numpy as np\n",
    "file_lenghts = [100, 110, 220, 330]\n",
    "cumulative_lengths = np.cumsum(file_lenghts)\n",
    "print(cumulative_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_idx(cumulative_lengths, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bstadt-env",
   "language": "python",
   "name": "bstadt-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
