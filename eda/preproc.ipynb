{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "raw_sources = glob.glob('../coca/text/text*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_general(fpath):\n",
    "\n",
    "    with open(fpath, 'r', encoding='utf-8') as file:\n",
    "        print('loading text from: ', fpath)\n",
    "        text = file.read()\n",
    "\n",
    "    texts = []\n",
    "    year = fpath.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    text = text.replace('<p>', ' ')\n",
    "    text = text.replace('<h>', ' ')\n",
    "    text = text.replace('@!', ' ')\n",
    "    text = text.replace('@ @ @ @ @ @ @ @ @ @', '[MASK_NOLOSS]')\n",
    "    text = text.split('@@')\n",
    "    for e in text:\n",
    "        if e.strip():\n",
    "            # Find the first whitespace and split into 2 parts\n",
    "            first_space_index = e.find(' ')\n",
    "            if first_space_index != -1:\n",
    "                docid = e[:first_space_index]\n",
    "                content = '[YEAR:{year}] '.format(year=year) + e[first_space_index + 1:]\n",
    "                texts.append(content)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenum_to_year = {}\n",
    "with open('../coca/sources.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    for line in file:\n",
    "        parts = line.split('\\t')\n",
    "        try:\n",
    "            filenum_to_year[parts[0]] = int(parts[1])\n",
    "        except Exception as e:\n",
    "            print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 1990, Count: 7109\n",
      "Year: 1991, Count: 7307\n",
      "Year: 1992, Count: 7696\n",
      "Year: 1993, Count: 8023\n",
      "Year: 1994, Count: 8251\n",
      "Year: 1995, Count: 8412\n",
      "Year: 1996, Count: 8111\n",
      "Year: 1997, Count: 8769\n",
      "Year: 1998, Count: 8841\n",
      "Year: 1999, Count: 9009\n",
      "Year: 2000, Count: 10142\n",
      "Year: 2001, Count: 9395\n",
      "Year: 2002, Count: 10055\n",
      "Year: 2003, Count: 10595\n",
      "Year: 2004, Count: 10069\n",
      "Year: 2005, Count: 9762\n",
      "Year: 2006, Count: 9937\n",
      "Year: 2007, Count: 9796\n",
      "Year: 2008, Count: 9361\n",
      "Year: 2009, Count: 9587\n",
      "Year: 2010, Count: 9814\n",
      "Year: 2011, Count: 11052\n",
      "Year: 2012, Count: 198053\n",
      "Year: 2013, Count: 9177\n",
      "Year: 2014, Count: 9490\n",
      "Year: 2015, Count: 9497\n",
      "Year: 2016, Count: 14800\n",
      "Year: 2017, Count: 15033\n",
      "Year: 2018, Count: 14032\n",
      "Year: 2019, Count: 14004\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "year_counts = Counter(filenum_to_year.values())\n",
    "for year, count in sorted(year_counts.items()):\n",
    "    print(f\"Year: {year}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenum_to_year['5028898']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_blog_web(fpath):\n",
    "\n",
    "    with open(fpath, 'r', encoding='utf-8') as file:\n",
    "        print('loading text from: ', fpath)\n",
    "        text = file.read()\n",
    "\n",
    "    texts = []\n",
    "    text = text.replace('<p>', ' ')\n",
    "    text = text.replace('<h>', ' ')\n",
    "    text = text.replace('&', ' ')\n",
    "    text = text.replace('@ @ @ @ @ @ @ @ @ @', '[MASK_NOLOSS]')\n",
    "    text = text.split('@@')\n",
    "    for e in text:\n",
    "        if e.strip():\n",
    "            # Find the first whitespace and split into 2 parts\n",
    "            first_space_index = e.find(' ')\n",
    "            if first_space_index != -1:\n",
    "                docid = e[:first_space_index]\n",
    "                text = e[first_space_index + 1:]\n",
    "                if docid in filenum_to_year:\n",
    "                    year = filenum_to_year[docid]\n",
    "                    content = '[YEAR:{year}] '.format(year=year) + e[first_space_index + 1:]\n",
    "                    texts.append(content)\n",
    "                else:\n",
    "                    print(f\"docid {docid} not found in filenum_to_year\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_file(fpath):\n",
    "\n",
    "\n",
    "    if 'acad' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'blog' in fpath:\n",
    "        texts = preproc_blog_web(fpath)\n",
    "    elif 'fic' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'mag' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'news' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'spok' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'tvm' in fpath:\n",
    "        texts = preproc_general(fpath)\n",
    "    elif 'web' in fpath:\n",
    "        texts = preproc_blog_web(fpath)\n",
    "\n",
    "\n",
    "    else:                \n",
    "        raise NotImplementedError(f\"Genre type not supported: {fpath}\")\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new special tokens\n",
    "additional_special_tokens = ['[MASK_NOLOSS]'] + ['[YEAR:{i}]'.format(i=i) for i in range(1900, 2025)]\n",
    "special_tokens_dict = {'additional_special_tokens': additional_special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_source(source):\n",
    "    sequence_length = 512\n",
    "    overlap = 128\n",
    "    step_size = sequence_length - overlap\n",
    "    for text in preproc_file(source):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        sequences = [tokens[i:i + sequence_length] for i in range(0, len(tokens), step_size)]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text from:  ./helivan-project-generation/bstadt/tlm/coca/text/text_acad_1990.txt\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenize_source(raw_sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 210 430 760]\n"
     ]
    }
   ],
   "source": [
    "def get_file_idx(cumulative_lengths, idx):\n",
    "    return int(np.searchsorted(cumulative_lengths, idx))\n",
    "\n",
    "import numpy as np\n",
    "file_lenghts = [100, 110, 220, 330]\n",
    "cumulative_lengths = np.cumsum(file_lenghts)\n",
    "print(cumulative_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_idx(cumulative_lengths, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
