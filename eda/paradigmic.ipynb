{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0a3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32a5842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bstadt/root/tlm/tlmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "loadstr = '/home/bstadt/root/tlm/models/tlm-2025-08-05_16-42-11/checkpoint-10500/'\n",
    "model = AutoModelForMaskedLM.from_pretrained(loadstr)\n",
    "tokenizer = AutoTokenizer.from_pretrained(loadstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5bbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_fills(phrase, model, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Get the top k most likely fills for a phrase with mask tokens.\n",
    "    \n",
    "    Args:\n",
    "        phrase: String with [MASK] tokens to fill\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        top_k: Number of top fills to return (default 5)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (fill_tokens, probability) for top k fills\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and find mask positions\n",
    "    inputs = tokenizer.encode_plus(phrase, add_special_tokens=False, return_tensors='pt')\n",
    "    mask_locs = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    n_masks = len(mask_locs)\n",
    "    \n",
    "    if n_masks == 0:\n",
    "        raise ValueError(\"No mask tokens found in phrase\")\n",
    "    \n",
    "    device = model.device\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0]  # Remove batch dimension\n",
    "        \n",
    "        # Get logits for each mask position\n",
    "        mask_logits = logits[mask_locs]  # Shape: (n_masks, vocab_size)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        mask_probs = F.softmax(mask_logits, dim=-1)\n",
    "        \n",
    "        # For multiple masks, we need to consider combinations\n",
    "        if n_masks == 1:\n",
    "            # Single mask case\n",
    "            top_probs, top_indices = torch.topk(mask_probs[0], top_k)\n",
    "            results = []\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                token = tokenizer.decode([idx.item()])\n",
    "                results.append((token, prob.item()))\n",
    "            return results\n",
    "        \n",
    "        else:\n",
    "            # Multiple masks - get top tokens for each position and combine\n",
    "            # This is a simplified approach - for exact top-k we'd need beam search\n",
    "            top_tokens_per_mask = []\n",
    "            for i in range(n_masks):\n",
    "                top_probs_i, top_indices_i = torch.topk(mask_probs[i], top_k)\n",
    "                top_tokens_per_mask.append([(tokenizer.decode([idx.item()]), prob.item()) \n",
    "                                          for prob, idx in zip(top_probs_i, top_indices_i)])\n",
    "            \n",
    "            # Generate combinations and compute joint probabilities\n",
    "            from itertools import product\n",
    "            combinations = list(product(*top_tokens_per_mask))\n",
    "            \n",
    "            results = []\n",
    "            for combo in combinations:\n",
    "                tokens = [token for token, _ in combo]\n",
    "                joint_prob = np.prod([prob for _, prob in combo])\n",
    "                fill_text = ' '.join(tokens)\n",
    "                results.append((fill_text, joint_prob))\n",
    "            \n",
    "            # Sort by probability and return top k\n",
    "            results.sort(key=lambda x: x[1], reverse=True)\n",
    "            return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad8b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1990, 2020))\n",
    "year_fills = ['[YEAR:{}]'.format(year) for year in years]\n",
    "year_fill_token_ids = [tokenizer.encode(e)[1] for e in year_fills]\n",
    "def lyear(phrase, model, tokenizer):\n",
    "    year_template = '[MASK] ' + phrase\n",
    "    input_ids = tokenizer.encode(year_template, add_special_tokens=False, return_tensors='pt')\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits[0][0]\n",
    "        year_sublogits = logits[year_fill_token_ids]\n",
    "        year_subprobs = F.softmax(year_sublogits, dim=0)\n",
    "\n",
    "    return years, year_sublogits, year_subprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f6da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uses(uses_path, word):\n",
    "    with open(uses_path, 'r', encoding='utf-8') as f:\n",
    "        uses_data = f.readlines()\n",
    "\n",
    "    uses_data = [line.strip() for line in uses_data if line.strip()]\n",
    "\n",
    "    print(f\"Loaded {len(uses_data)} lines from {uses_path}\")\n",
    "\n",
    "    import re\n",
    "    uses_data = [text for text in uses_data if re.search(r'\\b{}\\b'.format(word), text)]\n",
    "    print(f\"Filtered to {len(uses_data)} sentences containing the word '{word}'\")\n",
    "\n",
    "    use_loc = uses_data[0].index(word)\n",
    "    print(uses_data[0][use_loc-50:use_loc+50])\n",
    "    get_top_fills(uses_data[0].replace(word, '[MASK]'), model, tokenizer)\n",
    "\n",
    "    return uses_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487d9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def strip_year(text):\n",
    "    #removes the year token and returns the year as an int, as well as the text without the year token\n",
    "    year_pattern = r'\\[YEAR:(\\d+)\\]'\n",
    "    year = re.search(year_pattern, text)\n",
    "    if year:\n",
    "        return int(year.group(1)), re.sub(year_pattern, '', text)\n",
    "    else:\n",
    "        return None, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b99ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def get_posterior(phrase, model, tokenizer, exclude_text=None):\n",
    "    nucleus = [fill[0] for fill in get_top_fills(phrase, model, tokenizer, top_k=10)]\n",
    "    if exclude_text:\n",
    "        nucleus = [fill for fill in nucleus if exclude_text not in fill]\n",
    "    _, _, template_year_subprobs = lyear(phrase, model, tokenizer)\n",
    "    bayes_by_fill = []\n",
    "    for fill in nucleus:\n",
    "        filled_phrase = phrase.replace('[MASK]', fill)\n",
    "        years, _, fill_probs = lyear(filled_phrase, model, tokenizer)\n",
    "        bayes_factors = fill_probs/template_year_subprobs\n",
    "        bayes_by_fill.append(bayes_factors.detach().cpu().numpy())\n",
    "\n",
    "    posterior = np.stack(bayes_by_fill).T/np.sum(bayes_by_fill, axis=0, keepdims=True).T\n",
    "    return nucleus, posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "217c846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def aggregate_posteriors(posteriors):\n",
    "    word_to_mass = defaultdict(float)\n",
    "    for posterior in posteriors:\n",
    "        for word, mass in zip(posterior['nucleus'], posterior['posterior']):\n",
    "            word_to_mass[word] += mass\n",
    "    #uniform dist over documents\n",
    "    aggregated_posterior = {word: mass/len(posteriors) for word, mass in word_to_mass.items()}\n",
    "    return aggregated_posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d481cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(matrix, wordlist):\n",
    "    \n",
    "    # Create a graph from the matrix\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for word in wordlist:\n",
    "        G.add_node(word)\n",
    "\n",
    "    # Add edges based on the matrix (only add edges with significant weights)\n",
    "    threshold = 0.00  # Adjust this threshold as needed\n",
    "    for i in range(len(wordlist)):\n",
    "        for j in range(i+1, len(wordlist)):\n",
    "            if matrix[i, j] > threshold:\n",
    "                G.add_edge(wordlist[i], wordlist[j], weight=matrix[i, j])\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_coords = pca.fit_transform(matrix)\n",
    "\n",
    "    # Create initial positions dictionary from PCA coordinates\n",
    "    initial_pos = {node: pca_coords[i] for i, node in enumerate(wordlist)}\n",
    "\n",
    "    # Create force-directed layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50000, seed=42, scale=4, pos=initial_pos)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, width=0.5)\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='lightblue', alpha=0.7)\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "    plt.title('Force-Directed Network Visualization of Word-Cell Matrix')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return G, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b87e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_interactive_plot(G, pos, word):\n",
    "    # save as ego_plotly_hover.py and run\n",
    "    import json\n",
    "    import networkx as nx\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "\n",
    "    # Define the color palette\n",
    "    good_colors = ['#EA5526', '#4462BD', '#51915B', '#8064A2', '#E5B700']\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) build a sample graph\n",
    "    # -----------------------\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {n: i for i, n in enumerate(nodes)}\n",
    "    n_nodes = len(nodes)\n",
    "\n",
    "    # adjacency list as list-of-lists indexed by node index\n",
    "    adj = [[] for _ in range(n_nodes)]\n",
    "    for u, v in G.edges():\n",
    "        ui, vi = node_index[u], node_index[v]\n",
    "        adj[ui].append(vi)\n",
    "        adj[vi].append(ui)\n",
    "\n",
    "    # prepare edge segments grouped by which node they touch\n",
    "    edges_by_node = [[] for _ in range(n_nodes)]\n",
    "    edge_segments = []  # all edges (for base faint edges)\n",
    "    for u, v in G.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_segments.append(((x0, x1, None), (y0, y1, None)))  # None creates breaks between segments\n",
    "        ui, vi = node_index[u], node_index[v]\n",
    "        edges_by_node[ui].append((x0, x1, y0, y1))\n",
    "        edges_by_node[vi].append((x0, x1, y0, y1))\n",
    "\n",
    "    # flatten for base edge trace\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for xs, ys in edge_segments:\n",
    "        edge_x.extend(xs)\n",
    "        edge_y.extend(ys)\n",
    "\n",
    "    # node coordinates\n",
    "    node_x = [pos[n][0] for n in nodes]\n",
    "    node_y = [pos[n][1] for n in nodes]\n",
    "    node_text = [str(n) for n in nodes]\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) create plotly traces\n",
    "    # -----------------------\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        mode='lines',\n",
    "        line=dict(width=1, color='#CCCCCC'),\n",
    "        hoverinfo='none',\n",
    "        opacity=0.3,\n",
    "        name='edges'\n",
    "    )\n",
    "\n",
    "    # trace that will hold highlighted edges (initially empty)\n",
    "    highlight_edge_trace = go.Scatter(\n",
    "        x=[], y=[],\n",
    "        mode='lines',\n",
    "        line=dict(width=2, color='#6B79A3'),  # Thicker but same color as regular edges\n",
    "        hoverinfo='none',\n",
    "        opacity=0.3,\n",
    "        name='highlight_edges'\n",
    "    )\n",
    "\n",
    "    # Invisible node trace for hover detection only\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers',\n",
    "        marker=dict(size=20, color='rgba(0,0,0,0)', opacity=0),  # Invisible markers\n",
    "        hoverinfo='none',  # Remove tooltips\n",
    "        text=node_text,\n",
    "        name='nodes',\n",
    "        customdata=list(range(n_nodes))  # handy to get index in JS\n",
    "    )\n",
    "\n",
    "    # Background rectangles for text (will be added via shapes in layout)\n",
    "    # We'll create these dynamically based on text positions\n",
    "\n",
    "    # Text trace for node labels - now the main visual element\n",
    "    text_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='text',\n",
    "        text=node_text,\n",
    "        textfont=dict(size=18, color='black', family='Geist Mono'),\n",
    "        hoverinfo='none',  # Remove tooltips\n",
    "        name='text',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Create background rectangles for each text label\n",
    "    text_backgrounds = []\n",
    "    for i, (x, y, text) in enumerate(zip(node_x, node_y, node_text)):\n",
    "        # Estimate text width and height (rough approximation)\n",
    "        text_width = len(str(text)) * 0.011  # Approximate character width in plot coordinates\n",
    "        text_height = 0.025  # Approximate text height in plot coordinates\n",
    "        \n",
    "        text_backgrounds.append(dict(\n",
    "            type=\"rect\",\n",
    "            x0=x - text_width/2,\n",
    "            y0=y - text_height/2,\n",
    "            x1=x + text_width/2,\n",
    "            y1=y + text_height/2,\n",
    "            fillcolor=\"#FDF6E8\",\n",
    "            line=dict(width=0),\n",
    "            layer=\"below\"\n",
    "        ))\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, highlight_edge_trace, node_trace, text_trace],\n",
    "                    layout=go.Layout(\n",
    "                        showlegend=False,\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=0, l=0, r=0, t=0),\n",
    "                        plot_bgcolor='#FDF6E8',\n",
    "                        paper_bgcolor='#FDF6E8',\n",
    "                        font=dict(family='Geist Mono'),\n",
    "                        shapes=text_backgrounds\n",
    "                    ))\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) produce HTML and inject JS for hover behavior\n",
    "    # -----------------------\n",
    "    # Unique div id so we can target the plotly div in the JS\n",
    "    div_id = \"ego-network-plot\"\n",
    "\n",
    "    # adjacency and edges_by_node must be serializable to JS\n",
    "    adj_json = json.dumps(adj)\n",
    "    # For edges_by_node, create lists of segment coordinates as pairs to simplify JS handling\n",
    "    edges_by_node_js = []\n",
    "    for segs in edges_by_node:\n",
    "        # segs is list of (x0,x1,y0,y1)\n",
    "        segs_js = [[x0, x1, y0, y1] for (x0, x1, y0, y1) in segs]\n",
    "        edges_by_node_js.append(segs_js)\n",
    "    edges_by_node_json = json.dumps(edges_by_node_js)\n",
    "\n",
    "    # Build client-side JS: attaches to plot's div, listens for plotly_hover & plotly_unhover,\n",
    "    # updates node colors and the highlight-edge trace.\n",
    "    injected_js = f\"\"\"\n",
    "    <script>\n",
    "    (function(){{\n",
    "    const adj = {adj_json};\n",
    "    const edges_by_node = {edges_by_node_json};\n",
    "    const goodColors = {json.dumps(good_colors)};\n",
    "    const defaultColor = goodColors[0];\n",
    "    const highlightColor = goodColors[1];\n",
    "    const neighborColor = goodColors[2];\n",
    "\n",
    "    const gd = document.getElementById('{div_id}');\n",
    "    if(!gd) return;\n",
    "\n",
    "    // helper to set text colors\n",
    "    function colorText(hoverIndex) {{\n",
    "        const n = adj.length;\n",
    "        const colors = new Array(n).fill('black');\n",
    "        if(hoverIndex !== null) {{\n",
    "        colors[hoverIndex] = '#EA5526';  // Orange color from palette\n",
    "        adj[hoverIndex].forEach(i => colors[i] = '#EA5526');\n",
    "        }}\n",
    "        // update the text trace (trace index 3)\n",
    "        Plotly.restyle(gd, {{'textfont.color': [colors]}}, [3]);\n",
    "    }}\n",
    "\n",
    "    // helper to show edges connected to hoverIndex in trace index 1\n",
    "    function showHighlightEdges(hoverIndex) {{\n",
    "        if(hoverIndex === null) {{\n",
    "        Plotly.restyle(gd, {{x:[[]], y:[[]]}}, [1]);\n",
    "        return;\n",
    "        }}\n",
    "        const segs = edges_by_node[hoverIndex]; // [[x0,x1,y0,y1], ...]\n",
    "        // Flatten into x and y arrays with None breaks\n",
    "        let xs = [], ys = [];\n",
    "        segs.forEach(s => {{\n",
    "        xs.push(s[0]); xs.push(s[1]); xs.push(null);\n",
    "        ys.push(s[2]); ys.push(s[3]); ys.push(null);\n",
    "        }});\n",
    "        Plotly.restyle(gd, {{x:[xs], y:[ys]}}, [1]);\n",
    "    }}\n",
    "\n",
    "    gd.on('plotly_hover', function(eventData) {{\n",
    "        const pt = eventData.points && eventData.points[0];\n",
    "        if(!pt) return;\n",
    "        // Handle hover on either the invisible node trace or text trace\n",
    "        if(pt.fullData && (pt.fullData.name === 'nodes' || pt.fullData.name === 'text')) {{\n",
    "        const idx = pt.pointIndex;\n",
    "        colorText(idx);\n",
    "        showHighlightEdges(idx);\n",
    "        }}\n",
    "    }});\n",
    "\n",
    "    gd.on('plotly_unhover', function(eventData) {{\n",
    "        colorText(null);\n",
    "        showHighlightEdges(null);\n",
    "    }});\n",
    "\n",
    "    // ensure initial text colors are set\n",
    "    colorText(null);\n",
    "    }})();\n",
    "    </script>\n",
    "    \"\"\"\n",
    "\n",
    "    # Configuration to remove Plotly logo\n",
    "    config = {\n",
    "        'displayModeBar': False,\n",
    "        'displaylogo': False,\n",
    "    }\n",
    "\n",
    "    # get HTML string with config\n",
    "    html_str = pio.to_html(fig, full_html=True, include_plotlyjs='cdn', div_id=div_id, config=config)\n",
    "    # append injected JS before closing </body>\n",
    "    html_str = html_str.replace(\"</body>\", injected_js + \"</body>\")\n",
    "\n",
    "    out_path = \"{}_network_plot.html\".format(word)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_str)\n",
    "\n",
    "    print(f\"Wrote interactive ego-network HTML to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paradigmic_change(word, uses_path, model, tokenizer, subset_size=300):\n",
    "\n",
    "    #load data\n",
    "    uses_data = load_uses(uses_path, word)\n",
    "\n",
    "    #random subset\n",
    "    import random\n",
    "    random.seed(42) \n",
    "    selected_indices = random.sample(range(len(uses_data)), min(subset_size, len(uses_data)))\n",
    "    masked_texts = [uses_data[i].replace(word, '[MASK]', 1) for i in selected_indices]\n",
    "\n",
    "    #move model here to prevent weird cuda memory errors\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    #get posteriors\n",
    "    posteriors_by_year = {k: [] for k in range(1990, 2020)}\n",
    "    for text in tqdm(masked_texts):\n",
    "        year, text = strip_year(text)\n",
    "        if year:\n",
    "            nucleus, posterior = get_posterior(text, model, tokenizer)\n",
    "            posteriors_by_year[year].append({'nucleus': nucleus, 'posterior': posterior[year-1990], 'text':text})\n",
    "\n",
    "    #aggregate posteriors\n",
    "    aggregated_posteriors = {}\n",
    "    for year_bucket_start in range(1990, 2020, 5):\n",
    "        years_in_bucket = [year for year in range(year_bucket_start, year_bucket_start+5) if year in posteriors_by_year]\n",
    "        bucket_posteriors = []\n",
    "        for year in years_in_bucket:\n",
    "            bucket_posteriors.extend(posteriors_by_year[year])\n",
    "        \n",
    "        aggregated_posterior = aggregate_posteriors(bucket_posteriors)\n",
    "        aggregated_posteriors[year_bucket_start] = aggregated_posterior\n",
    "\n",
    "    #top words by bucket\n",
    "    top_words_by_bucket = {}\n",
    "    for year_bucket, posterior in aggregated_posteriors.items():\n",
    "        # Sort by posterior probability and get top 20\n",
    "        filtered = {k: v for k, v in posterior.items() if 'cell' not in k.lower()}\n",
    "        filtered = {k: v for k, v in filtered.items() if sum(c.isalpha() for c in k) >= 2}\n",
    "\n",
    "        sorted_words = sorted(filtered.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        top_words_by_bucket[year_bucket] = sorted_words\n",
    "\n",
    "    #create wordlist\n",
    "    wordset = set()\n",
    "    for year_bucket, posterior in top_words_by_bucket.items():\n",
    "        for elem in posterior:\n",
    "            wordset.add(elem[0])\n",
    "    og_wordlist = list(wordset)\n",
    "    wordlist = og_wordlist + [word+':'+str(year)+'-'+str(year+5) for year in range(1990, 2020, 5)]\n",
    "\n",
    "    #create use matrix\n",
    "    bipartite = np.zeros((len(wordlist) - len(og_wordlist), len(og_wordlist)))\n",
    "    for i, year in enumerate(top_words_by_bucket.keys()):\n",
    "        posterior = top_words_by_bucket[year]\n",
    "        for e in posterior:\n",
    "            word = e[0]\n",
    "            mass = e[1]\n",
    "            if word in og_wordlist:\n",
    "                bipartite[i, og_wordlist.index(word)] = mass\n",
    "\n",
    "    wordsims = bipartite @ bipartite.T  # shape: (num_U_nodes, num_U_nodes)\n",
    "\n",
    "    matrix = np.zeros((len(wordlist), len(wordlist)))\n",
    "    matrix[len(og_wordlist):, len(og_wordlist):] = wordsims\n",
    "    matrix[len(og_wordlist):, :len(og_wordlist)] = bipartite\n",
    "    matrix[:len(og_wordlist), len(og_wordlist):] = bipartite.T\n",
    "\n",
    "    #create and plot graph\n",
    "    G, pos = plot_graph(matrix, wordlist)\n",
    "\n",
    "    #write interactive plot\n",
    "    write_interactive_plot(G, pos, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46245584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 135269 lines from cell_uses.txt\n",
      "Filtered to 42990 sentences containing the word 'cell'\n",
      "il. The naked dead, all in [MASK_NOLOSS] the jail cell. They said nothing. But they were mute with d\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'model' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mparadigmic_change\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcell\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcell_uses.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mparadigmic_change\u001b[39m\u001b[34m(word, uses_path, subset_size)\u001b[39m\n\u001b[32m     10\u001b[39m masked_texts = [uses_data[i].replace(word, \u001b[33m'\u001b[39m\u001b[33m[MASK]\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m selected_indices]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#move model here to prevent weird cuda memory errors\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mmodel\u001b[49m.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#get posteriors\u001b[39;00m\n\u001b[32m     16\u001b[39m posteriors_by_year = {k: [] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1990\u001b[39m, \u001b[32m2020\u001b[39m)}\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'model' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "paradigmic_change('cell', 'cell_uses.txt', model, tokenizer, subset_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec767d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
